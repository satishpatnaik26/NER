{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**News Classification by LSTM**\n",
    "\n",
    "In this notebook, we will try to classify news merely from the language associated with it. We just use its headline and short description to classify the news's category. One thing we intentionally avoid is the author's name due to tendency of certain author to write articles on particular topics.\n",
    "\n",
    "In general this notebook is comprised of some sections which are:\n",
    "1. Preparing data\n",
    "2. Building the model\n",
    "3. Training the model\n",
    "4. User input\n",
    "\n",
    "We use some components here to name a few:\n",
    "* Torchtext library\n",
    "* Pre-trained word embedding\n",
    "* LSTM network architecture\n",
    "* Bidirectional LSTM\n",
    "* Multi-layered LSTM\n",
    "* Regularization\n",
    "* Adam optimizer\n",
    "* Cross-entropy loss function for classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing Data**\n",
    "\n",
    "We use Torchtext library to pre-process our data. Torchtext simplifies text data pre-processing that includes reading data, tokenizing, converting into tensors, and building vocabulary to be easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first we specify what our data comprises of. We decide that our data comprises of TEXT which are the news' headlines and short descriptions, as well as LABEL which is the news' category. Here we tokenize the text using [spacy](https://spacy.io/?source=post_page---------------------------) tokenizer and to make all the words use lower case. While we keep the entire LABEL as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy', lower = True)\n",
    "LABEL = data.LabelField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use TabularDataset for json type file here. We extract the entirety of our data into something like dictionary with three keys, 'headline', 'desc', and 'category' that corresponds to each news' headline, short description, and category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = data.TabularDataset(\n",
    "    path='../input/News_Category_Dataset_v2.json', format='json',\n",
    "    fields={'headline': ('headline', TEXT),\n",
    "            'short_description' : ('desc', TEXT),\n",
    "             'category': ('category', LABEL)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further split our dataset into training set trn, validation set vld, and test set tst using seed for reproducible result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "SEED = 1234\n",
    "\n",
    "trn, vld, tst = news.split(split_ratio=[0.7, 0.2, 0.1], random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check an example of our data. It should comprises parsed headline, description, and the associated category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'headline': [\"'\", 'train', 'hard', ',', 'land', 'soft', \"'\"],\n",
       " 'desc': ['runners',\n",
       "  'will',\n",
       "  'appreciate',\n",
       "  'that',\n",
       "  'the',\n",
       "  'sproing',\n",
       "  'trainer',\n",
       "  'was',\n",
       "  'designed',\n",
       "  'with',\n",
       "  'them',\n",
       "  'in',\n",
       "  'mind',\n",
       "  'as',\n",
       "  'a',\n",
       "  'way',\n",
       "  'to',\n",
       "  'build',\n",
       "  'endurance',\n",
       "  'and',\n",
       "  'strength',\n",
       "  'without',\n",
       "  'the',\n",
       "  'pain',\n",
       "  'that',\n",
       "  'can',\n",
       "  'come',\n",
       "  'with',\n",
       "  'pounding',\n",
       "  'the',\n",
       "  'pavement',\n",
       "  '.'],\n",
       " 'category': 'HEALTHY LIVING'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(trn[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our vocabulary from our datasets and convert it into vectors from glove. From there we check how many vocabularies we have from our text and how many categories we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [00:43, 20.0MB/s]                           \n",
      " 99%|█████████▉| 397689/400000 [00:17<00:00, 23267.97it/s]"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(trn, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78629\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(TEXT.vocab))\n",
    "print(len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7f4dc075fd90>, {'POLITICS': 0, 'WELLNESS': 1, 'ENTERTAINMENT': 2, 'TRAVEL': 3, 'STYLE & BEAUTY': 4, 'PARENTING': 5, 'HEALTHY LIVING': 6, 'QUEER VOICES': 7, 'FOOD & DRINK': 8, 'BUSINESS': 9, 'COMEDY': 10, 'SPORTS': 11, 'BLACK VOICES': 12, 'HOME & LIVING': 13, 'PARENTS': 14, 'THE WORLDPOST': 15, 'WEDDINGS': 16, 'DIVORCE': 17, 'WOMEN': 18, 'IMPACT': 19, 'CRIME': 20, 'MEDIA': 21, 'WEIRD NEWS': 22, 'GREEN': 23, 'WORLDPOST': 24, 'RELIGION': 25, 'STYLE': 26, 'SCIENCE': 27, 'TASTE': 28, 'WORLD NEWS': 29, 'TECH': 30, 'MONEY': 31, 'ARTS': 32, 'FIFTY': 33, 'GOOD NEWS': 34, 'ARTS & CULTURE': 35, 'ENVIRONMENT': 36, 'COLLEGE': 37, 'LATINO VOICES': 38, 'CULTURE & ARTS': 39, 'EDUCATION': 40})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let's wrap out data to get the relevant iterator for our training, validation, as well as test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (trn, vld, tst), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort_key= lambda x: len(x.headline), \n",
    "    sort_within_batch= False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the Model**\n",
    "\n",
    "In this section, we define our model. Since we are trying to classify the news based on its headline and short description that are in the form of sentences or paragraphs, we are going to use sequential model that is LSTM (Long Short Term Memory). More specifically, we use bidirectional and two-layered LSTM layer hopefully to get better accuracy for our prediction. We also implement regularization by using dropout during our forward pass. In this model, we specifically split the processing for the headline and short description and concatenate them before final processing to get the prediction of our news' category. The detail can be seen in the diagram below:\n",
    "\n",
    "![](https://i.imgur.com/6nXjqx8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm_head = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = bidirectional, dropout = dropout)\n",
    "        \n",
    "        self.lstm_desc = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = bidirectional, dropout = dropout)\n",
    "        \n",
    "        self.fc_head = nn.Linear(hidden_dim * 2, 100)\n",
    "        \n",
    "        self.fc_desc = nn.Linear(hidden_dim * 2, 100)\n",
    "        \n",
    "        self.fc_total = nn.Linear(200, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "                \n",
    "    def forward(self, headline, description):\n",
    "                        \n",
    "        embedded_head = self.dropout(self.embedding(headline))\n",
    "        \n",
    "        embedded_desc = self.dropout(self.embedding(description))\n",
    "                                    \n",
    "        output_head, (hidden_head, cell_head) = self.lstm_head(embedded_head)\n",
    "        \n",
    "        output_desc, (hidden_desc, cell_desc) = self.lstm_desc(embedded_desc)\n",
    "        \n",
    "        hidden_head = self.dropout(torch.cat((hidden_head[-2, :, :], hidden_head[-1, :, :]), dim = 1))\n",
    "        \n",
    "        hidden_desc = self.dropout(torch.cat((hidden_desc[-2, :, :], hidden_desc[-1, :, :]), dim = 1))\n",
    "        \n",
    "        full_head = self.fc_head(hidden_head)\n",
    "        \n",
    "        full_desc = self.fc_desc(hidden_desc)\n",
    "        \n",
    "        hidden_total = torch.cat((full_head, full_desc), 1)\n",
    "        \n",
    "        return self.fc_total(hidden_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our model and check how many parameters we are training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.2\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 12,594,029 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, replace the initial weights of the embedding layers with the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([78629, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6273, -0.1902,  1.6680,  ..., -0.8070, -1.1826, -1.5124],\n",
       "        [ 1.9585,  1.7921,  0.9731,  ...,  1.0562,  0.2050, -1.5183],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.1933,  2.2161,  1.3552,  ..., -0.5998,  0.3130, -0.4366],\n",
       "        [ 0.2764, -0.1016,  0.2038,  ...,  0.5003, -0.8798,  1.3708],\n",
       "        [ 0.4287,  0.3359,  1.2329,  ...,  0.2169, -0.7110, -0.2164]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose Adam algorithm as our optimizer, as well as cross entropy loss for our loss function since we are doing classification problem with multiple categories. We also define the function to calculate accuracy of our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the training and evaluate part of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                        \n",
    "        predictions = model(batch.headline, batch.desc).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.category)\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.category)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            \n",
    "            predictions = model(batch.headline, batch.desc).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.category)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.category)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our model. We will train it for five epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 397689/400000 [00:30<00:00, 23267.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 51s\n",
      "\tTrain Loss: 1.757 | Train Acc: 52.43%\n",
      "\t Val. Loss: 1.372 |  Val. Acc: 61.53%\n",
      "Epoch: 02 | Epoch Time: 1m 51s\n",
      "\tTrain Loss: 1.230 | Train Acc: 64.79%\n",
      "\t Val. Loss: 1.246 |  Val. Acc: 64.36%\n",
      "Epoch: 03 | Epoch Time: 1m 51s\n",
      "\tTrain Loss: 1.030 | Train Acc: 69.83%\n",
      "\t Val. Loss: 1.209 |  Val. Acc: 66.21%\n",
      "Epoch: 04 | Epoch Time: 1m 51s\n",
      "\tTrain Loss: 0.874 | Train Acc: 73.82%\n",
      "\t Val. Loss: 1.255 |  Val. Acc: 65.67%\n",
      "Epoch: 05 | Epoch Time: 1m 51s\n",
      "\tTrain Loss: 0.738 | Train Acc: 77.38%\n",
      "\t Val. Loss: 1.303 |  Val. Acc: 65.61%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'news_classification_model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we test it with our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.316 | Test Acc: 65.65%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User Input**\n",
    "\n",
    "In this section, we let ourself to put our own input and let the model predict the news' categories beyond the dataset. For consistencies, we will use news from Huffington Post and try to get its category predicted. Make sure that the first input is the headline and the second input is the short description of the article.\n",
    "\n",
    "News can be obtained from [here](https://www.huffpost.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_category(model, head, desc):\n",
    "    model.eval()\n",
    "    head = head.lower()\n",
    "    desc = desc.lower()\n",
    "    tokenized_head = [tok.text for tok in nlp.tokenizer(head)]\n",
    "    tokenized_desc = [tok.text for tok in nlp.tokenizer(desc)]\n",
    "    indexed_head = [TEXT.vocab.stoi[t] for t in tokenized_head]\n",
    "    indexed_desc = [TEXT.vocab.stoi[t] for t in tokenized_desc]\n",
    "    tensor_head = torch.LongTensor(indexed_head).to(device)\n",
    "    tensor_desc = torch.LongTensor(indexed_desc).to(device)\n",
    "    tensor_head = tensor_head.unsqueeze(1)\n",
    "    tensor_desc = tensor_desc.unsqueeze(1)\n",
    "    prediction = model(tensor_head, tensor_desc)\n",
    "    max_pred = prediction.argmax(dim=1)\n",
    "    return max_pred.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News headline: Trump’s Art Of Distraction\n",
    "\n",
    "News short description: The conversation surrounding Trump’s latest racist rants has provoked us to revisit author Toni Morrison’s 1975 keynote address at Portland State University on the true purpose of racism.\n",
    "\n",
    "Correct category: Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category is: 0 = POLITICS\n"
     ]
    }
   ],
   "source": [
    "pred = predict_category(model, \"Trump’s Art Of Distraction\", \"The conversation surrounding Trump’s latest racist rants has provoked us to revisit author Toni Morrison’s 1975 keynote address at Portland State University on the true purpose of racism..\")\n",
    "print(f'Predicted category is: {pred} = {LABEL.vocab.itos[pred]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News headline: Indiana Cop Apologizes After Accusing McDonald’s Worker Of Eating His Sandwich\n",
    "\n",
    "News short description: The Marion County sheriff’s deputy forgot he had taken a bite out of his McChicken earlier that day, authorities said.\n",
    "\n",
    "Correct category: U.S. News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category is: 20 = CRIME\n"
     ]
    }
   ],
   "source": [
    "pred = predict_category(model, \"Indiana Cop Apologizes After Accusing McDonald’s Worker Of Eating His Sandwich\", \"The Marion County sheriff’s deputy forgot he had taken a bite out of his McChicken earlier that day, authorities said.\")\n",
    "print(f'Predicted category is: {pred} = {LABEL.vocab.itos[pred]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News headline: Kyle ‘Bugha’ Giersdorf, 16, Wins Fortnite World Cup And Takes Home $ 3 Million Prize\n",
    "\n",
    "News short description: Fortnite has nearly 250 million registered players and raked in an estimated $2.4 billion last year.\n",
    "\n",
    "Correct category: Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category is: 11 = SPORTS\n"
     ]
    }
   ],
   "source": [
    "pred = predict_category(model, \"Kyle ‘Bugha’ Giersdorf, 16, Wins Fortnite World Cup And Takes Home $ 3 Million Prize\", \"Fortnite has nearly 250 million registered players and raked in an estimated $2.4 billion last year.\")\n",
    "print(f'Predicted category is: {pred} = {LABEL.vocab.itos[pred]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "This notebook was created thanks to the two references below.\n",
    "* http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "* https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
